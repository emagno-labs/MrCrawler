\documentclass[a4paper]{article}

\usepackage[brazil]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{hyperref}

\title{BCC 4º Período - Projeto Interativo IV}
\author{Eryckson Magno\\
	Hamilton Santana\\
    Lucas M. Ribeiro\\
    Ivan Probst}
    
\begin{document}
\maketitle

\section{Introdução}

De uma forma genérica podemos descrever um \emph{web crawler} como sendo um programa, ou rotina, automáticos e metódicos que varrem um conjunto de dados na Web realizando a coleta de dados para posterior análise.

Se não seu uso mais comum sua utilização para indexação de páginas para motores de busca é talvez o mais visível. Porém estes robozinhos podem ser utilizados para diversas outras tarefas, como por exemplo a manutenção de web sites, onde são utilizados para apontar páginas com HTML mal formado ou links quebrados.

Um fator mais obscuro, mas muito importante é que \emph{web crawlers} são partes fundamentais em ferramentas de varredura e auditoria de segurança em websites, conhecidas como \emph{Web Application Security Scanner}.

Tais ferramentas também são ferramentas automáticas que testam aplicações web para problemas de segurança comuns, como o \emph{Cross-Site Scripting} (XSS), injeções (\emph{SQL Injection, XPath Injection\dots}), configurações inseguras dentre outras. Elas varrem uma aplicação web em busca de fraquezas e vulnerabilidades, realizando tanto manipulações nas mensagens HTTP ou as inspecionando a procura de atributos/valores suspeitos ou sensíveis.

\section{Ferramentas de Mercado}

Uma grande quantidade de ferramentas de varredura de segurança estão disponíveis no mercado e seus sabores também são diversivicados: encontramos soluções geniais open-soure ou de código fechado porém free, como também \$oluçõe\$ poderosa\$ com recursos muito interessantes.

Algumas soluções free ou open-source:

\begin{enumerate}
\item Zed Attack Proxy \href{https://www.owasp.org/index.php/OWASP_Zed_Attack_Proxy_Project}{(www.owasp.org)}
\item w3af \href{http://w3af.org/}{(w3af.org)}
\item Acunetix \href{http://www.acunetix.com/}{(www.acunetix.com)}
\item Websecurify \href{http://www.websecurify.com/}{(www.websecurify.com)}
\item IronWASP \href{http://ironwasp.org/}{(ironwasp.org)}
\item wapiti \href{http://wapiti.sourceforge.net/}{(wapiti.sourceforge.net)}
\end{enumerate}

Algumas \$oluçõe\$ comerciai\$:

\begin{enumerate}
\item Nikto Web Scanner \href{http://www.cirt.net/nikto2}{(www.cirt.net/nikto2)}
\item Burp Suite \href{http://portswigger.net/burp/}{(portswigger.net/burp)}
\end{enumerate}

O uso efetivo de ferramentas como estas são partes cruciais de processos de adequação e/ou manutenção de aplicações no que diz respeito a segurança, e seu uso regular (e bem executado) são obrigatórios para obedecer a requerimentos de certificações como o \emph{Payment Card Industry Data Security Standard} (PCI-DSS) - que é uma certificação de adequação a normas intenacionais na recepção e manuseio de dados de cartão de crédito.

\section{Proposição}

O mundo da segurança da informação é magnífico e bem abrangente, mas isto implica afirmar que construir uma ferramenta de varredura focada em segurança é um desafio enorme (em complexidade, funcionalidade e abrangência) e por isso foge do escopo do Projeto Interativo IV (PI IV).

\subsection{O que pode ser feito}

Entretanto, se construir uma \emph{Web Application Security Scanner} completa é algo muito amplo, o que pode ser feito é construir um \emph{Web Crawler} baseado nas funcionalidades necessárias para uma aplicação de varredura de segurança:
 
\begin{enumerate}

\item Identificar vulnerabilidades contra um alvo específico,
\item Analisar todo o tráfego de dados que passa pela ferramenta,
\item Detectar páginas de erro e respostas 404 customizadas,
\item Investigar redirecionamentos (HTTP, em meta dados e via JavaScript),
\item Identificar e coletar cookies,
\item Coletar informações sensíveis como URLs, formulários HTML, parâmetros HTML, comentários de código e afins,
\item \emph{Parsing} (mapeamento) de dados \emph{PlainText}, HTML e XML,
\item Identificar HTML mal formado,
\item Identificar correta parametrização de \emph{autocomplete} em campos sensíveis,

\dots e desejáveis \dots

\item \emph{Parsing} (mapeamento) de JavaScript e CSS,
\item Suportar aplicações AJAX (enviar \emph{XmlHttpRequests} automaticamente),
\item Prover um método para \emph{treinar} o crawler (identificar vulnerabilidades de forma passiva),
\item Verificar cookie de sessão (se está configurado de forma segura)
\item Configuração para "ajuste fino" da varredura.

\end{enumerate}

Entendendo que o projeto é uma \emph{prova de conceito}, pode ainda ser utilizado outro guia, mais voltado aos testes de segurança (\emph{Penetration Tests} por exemplo), que enumera os principais tópicos de vulnerabilidades dos acontecimentos mais recentes: o \href{http://www.owasp.org}{OWASP Top 10 2013}. \cite{lamport94}

\begin{enumerate}
\item Injection,
\item Broken Authentication and Session Management,
\item Cross-Site scripting (XSS),
\item Insecure Direct Object References,
\item Security Misconfiguration,
\item Sensitive Data Exposure,
\item Missing Function Level Access Control,
\item Cross-Site Request Forgery (CSRF),
\item Using Know Vulnerable Components,
\item Unvalidated Redirects and Forwards.
\end{enumerate}

Assim, outras informações relacionadas a este \emph{Top 10} (dados de cabeçalho de mensagens HTTP, IDs de sessão, mensagens de erro relacionadas aos retornos HTTP 5xx, acesso indevido à árvore de diretórios, arquivos padrão ou de backup, \dots) que podem ser coletadas e que são sensíveis de análise poderiam vir a incrementar a lista de \emph{desejáveis} para o crawler.

\subsection{Como pode ser visualizado}

Após a coleta de dados, será disponibilizada uma interface para visualização e análise dos dados voltada para o conceito \emph{NUI (Natural User Interface)}, promovendo uma maior naturalidade para o trabalho com a manipulação dos dados alvos de estudo.

Sendo assim, para interface será aplicado o uso de \emph{dashboards} que atuarão como painéis de controle, permitindo um aprofundamento na combinação e análise dos diferentes dados expostos.

\begin{thebibliography}{9}

\bibitem{lamport94}
  \emph{The Open Web Application Security Project (OWASP)}
  http://www.owasp.org

\end{thebibliography}

\end{document}